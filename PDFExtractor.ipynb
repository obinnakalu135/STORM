{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeebf03-3d65-4845-bcad-c78e6e524304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Extractor\n",
    "# Obinna Kalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e0a8ffd-6640-4c80-a157-c89db6bff1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/new/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from PIL import ImageFont, ImageDraw\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk.data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import termcolor\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e9ed14-cdd1-4821-8ddc-dc5241b06f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "data_dir = \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850b064f-19c2-4be8-9c87-b614a2f52e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1450', '1457', '1468', '1461', '1466', '1459', '1467', '1458', '1460', '1456', '1469', '1451', '1523', '.DS_Store', '1343', '1513', '1335', '1508', '1530', '1506', '1501', '1487', '1473', '1474', '1480', '1488', '1481', '1475', '1472', '1486', '1454', '1453', '1465', '1496', '1462', '1463', '1464', '1452', '1455', '1439', '1527', '1516', '1511', '1510', '1528', '1738', '1339', '1737', '1448', '1477', '1483', '1484', '1470', '1479', '1446', '1478', '1447', '1471', '1485', '1449', '1482', '1476']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(data_dir)\n",
    "print (files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5a5601c-bbfe-46dc-8bbf-9dd864a36c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/new/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download NLTK resources (download only once)\n",
    "nltk.download('punkt')  # Download sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ad41547-fddf-4d4d-b06e-da9f3f1612eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_pdf(pdf_path, password=None):\n",
    "  \"\"\"\n",
    "  Extracts text, word counts, and phrase counts from a PDF.\n",
    "\n",
    "  Args:\n",
    "      pdf_path (str): Path to the PDF file.\n",
    "      password (str, optional): Password for the PDF (if password-protected). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "      tuple: A tuple containing (text, word_counts, phrase_counts) or (None, None, None) if an error occurs.\n",
    "          - text (str): Extracted text from the PDF.\n",
    "          - word_counts (Counter): Counter object containing word frequencies.\n",
    "          - phrase_counts (Counter): Counter object containing bigram (2-word phrase) frequencies.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "      text = \"\"\n",
    "      # Extract text from PDF\n",
    "      with open(pdf_path, 'rb') as f:\n",
    "          reader = PdfReader(f, password=password)  # Provide password if needed\n",
    "          for page in reader.pages:\n",
    "              text += page.extract_text()\n",
    "\n",
    "      # Preprocess text (optional)\n",
    "      sentences = nltk.sent_tokenize(text)  # Tokenize into sentences\n",
    "      words = [w.lower() for w in nltk.word_tokenize(text) if w.isalpha()]  # Tokenize, lowercase, filter alphanumeric\n",
    "\n",
    "      # Remove stop words (optional)\n",
    "      stop_words = set(stopwords.words('english'))\n",
    "      words = [w for w in words if w not in stop_words]\n",
    "\n",
    "      # Create word and phrase frequency counts\n",
    "      word_counts = Counter(words)\n",
    "\n",
    "      # Consider using stemming/lemmatization for broader model coverage (optional)\n",
    "      # stemmed_words = [nltk.PorterStemmer().stem(w) for w in words]  # Example using Porter stemmer\n",
    "      # word_counts = Counter(stemmed_words)\n",
    "\n",
    "      phrase_counts = Counter(ngrams(words, 2))  # Count bigrams (2-word phrases)\n",
    "\n",
    "      return text, word_counts, phrase_counts\n",
    "  except (IOError, PyPDF2.errors.PdfReaderError) as e:\n",
    "      print(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "      return None, None, None\n",
    "\n",
    "def generate_wordcloud(text, filename, colormap='bright'):\n",
    "    \"\"\"\n",
    "    Generates a colorized word frequency list using termcolor.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text content for word frequency analysis.\n",
    "        filename (str): Filename for reference (used for display title).\n",
    "        colormap (str, optional): Colormap for termcolor (default: 'bright').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create word frequency dictionary\n",
    "    word_counts = Counter(nltk.word_tokenize(text.lower()))\n",
    "    max_count = max(word_counts.values())\n",
    "\n",
    "    # Display word cloud with filename reference\n",
    "    print(f\"\\n**Word Cloud for {filename}**\\n\")\n",
    "\n",
    "    for word, count in word_counts.most_common():\n",
    "        # Color weighting based on frequency (higher frequency = brighter color)\n",
    "        color_weight = int(count * 255 / max_count)\n",
    "        colored_word = colored(word, color='red', attrs=['bold'])  # Example using 'red'\n",
    "        print(f\"{colored_word:<20} - {count}\")  # Align words and format output\n",
    "\n",
    "def find_pdfs_with_phrase(phrase, pdf_data):\n",
    "  \"\"\"\n",
    "  Finds all PDFs containing a specific phrase.\n",
    "\n",
    "  Args:\n",
    "      phrase (str): The phrase to search for.\n",
    "      pdf_data (dict): Dictionary containing processed PDF data.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of tuples containing (pdf_path, phrase_count) for all PDFs with the phrase.\n",
    "  \"\"\"\n",
    "\n",
    "  matching_pdfs = []\n",
    "  for pdf_path, data in pdf_data.items():\n",
    "    if phrase in data[2]:  # Check if phrase exists in phrase_counts dictionary\n",
    "      phrase_count = data[2][phrase]\n",
    "      matching_pdfs.append((pdf_path, phrase_count))\n",
    "  return matching_pdfs\n",
    "\n",
    "def find_top_phrases(phrase, pdf_data):\n",
    "    \"\"\"\n",
    "    Finds the top 10 PDFs based on the frequency of a specific phrase.\n",
    "\n",
    "    Args:\n",
    "        phrase (str): The phrase to search for.\n",
    "        pdf_data (dict): Dictionary containing processed PDF data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing (pdf_path, phrase_count) for the top 10 PDFs.\n",
    "    \"\"\"\n",
    "\n",
    "    phrase_counts = [data[2][phrase] for data in pdf_data.values() if phrase in data[2]]  # Extract phrase count for each PDF\n",
    "    top_10_pdfs = sorted(zip(pdf_data.keys(), phrase_counts), key=lambda x: x[1], reverse=True)[:10]  # Sort and get top 10\n",
    "    return top_10_pdfs\n",
    "\n",
    "\n",
    "def load_dataset(data_dir=\"files\"):\n",
    "    \"\"\"\n",
    "    Loads the PDF dataset from a specified directory and prints the path for each file.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str, optional): The directory containing the PDF files. Defaults to \"files\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing (pdf_path, label).\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the PDF files in the dataset path\n",
    "    for file in os.listdir(data_dir):\n",
    "        subfolder = os.path.join(data_dir, file)\n",
    "        if os.path.isdir(subfolder):\n",
    "            # Iterate over images in the subfolder\n",
    "            for pdf_file in os.listdir(subfolder):\n",
    "                if pdf_file.endswith('.pdf'):\n",
    "                    # Provide the full path to the PDF file\n",
    "                    pdf_path = os.path.join(subfolder, pdf_file)\n",
    "                    label = file  # Assuming label is based on subfolder name\n",
    "\n",
    "                    # Print the file path\n",
    "                    print(pdf_path)\n",
    "\n",
    "                    images.append(pdf_path)\n",
    "                    labels.append(label)\n",
    "\n",
    "    return list(zip(images, labels))  # Return a list of tuples (pdf_path, label)\n",
    "\n",
    "def find_relevant_pdfs(pdf_paths, search_terms):\n",
    "  \"\"\"\n",
    "  Finds PDFs in the list that are most relevant to the given search terms.\n",
    "\n",
    "  Args:\n",
    "      pdf_paths (list): List of paths to PDF files.\n",
    "      search_terms (list): List of search terms (words or phrases).\n",
    "\n",
    "  Returns:\n",
    "      list: List of dictionaries containing information about relevant PDFs.\n",
    "          Each dictionary has the following keys:\n",
    "              - path (str): Path to the PDF file.\n",
    "              - score (float): Relevancy score based on search term frequency.\n",
    "  \"\"\"\n",
    "\n",
    "  relevant_pdfs = []\n",
    "  for pdf_path in pdf_paths:\n",
    "      text, word_counts, _ = process_pdf(pdf_path)  # Phrase counts not used here, optional for future use\n",
    "      if text:\n",
    "          score = 0\n",
    "          # Search for all search terms (consider stemming/lemmatization if used earlier)\n",
    "          for term in search_terms:\n",
    "              score += word_counts.get(term.lower(), 0)  # Count occurrences of each search term (lowercase)\n",
    "          relevant_pdfs.append({\n",
    "              \"path\": pdf_path,\n",
    "              \"score\": score\n",
    "          })\n",
    "  # Sort PDFs by score in descending order (most relevant first)\n",
    "  relevant_pdfs.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "  return relevant_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80add3fd-c9c2-4f04-9bba-5b05a7a0149d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/1450/Demertzis et al_2023_Federated Auto-Meta-Ensemble Learning Framework for AI-Enabled Military.pdf\n",
      "files/1457/Mao et al_2022_Trustworthy AI Solutions for Cyberbiosecurity Challenges in Water Supply Systems.pdf\n",
      "files/1468/Paul_2023_A survey of technologies supporting design of a multimodal interactive robot.pdf\n",
      "files/1461/Xiao et al_2022_Guest Editorial.pdf\n",
      "files/1466/Miljković_Beriša_2023_Application of artificial intelligence in modern warfare.pdf\n",
      "files/1459/Schuett_2022_Three lines of defense against risks from AI.pdf\n",
      "files/1467/Nalin_Tripodi_2023_Future Warfare and Responsibility Management in the AI-based Military.pdf\n",
      "files/1458/Moreno et al_2022_The ethics of AI-assisted warfighter enhancement research and experimentation.pdf\n",
      "files/1460/Wang_Chapman_2022_Risk-averse autonomous systems.pdf\n",
      "files/1456/Lee_Jang_2022_Necessity of establishing an open source military R&D platform to promote AI.pdf\n",
      "files/1469/Rashid et al_2023_Artificial Intelligence in the Military.pdf\n",
      "files/1451/Johnson_2023_Automating the OODA loop in the age of intelligent machines.pdf\n",
      "files/1523/Taddeo and Blanchard - 2022 - A Comparative Analysis of the Definitions of Auton.pdf\n",
      "files/1343/Rivera et al. - 2024 - Escalation Risks from Language Models in Military .pdf\n",
      "files/1513/Longpre et al. - 2022 - Lethal autonomous weapons systems & artificialinte.pdf\n",
      "files/1335/Chrvalová - 2022 - LETHAL AUTONOMOUS WEAPONS SYSTEMS.pdf\n",
      "files/1508/Androschuk - 2023 - The level of trust in artificial intelligence an .pdf\n",
      "files/1530/Falletti and Gallese - 2022 - Ethical and Legal Limits to the Diffusion of Self-.pdf\n",
      "files/1506/Kahn - 2022 - Lethal autonomous weapon systems and respect for h.pdf\n",
      "files/1501/Taddeo and Blanchard - 2022 - Accepting Moral Responsibility for the Actions of .pdf\n",
      "files/1487/Fazelnia et al. - 2022 - Attacks, Defenses, And Tools A Framework To Facil.pdf\n",
      "files/1473/Seyed jalal Dehghani Firozabadi_Saeed Chehrazad_2023_Artificial Intelligence and Problematization of National Security Topics.pdf\n",
      "files/1474/Shimaa S. Mohamed et al_2023_Neutrosophic MCDM Methodology for Risk Assessment of Autonomous Underwater.pdf\n",
      "files/1480/Gunneflo and Noll - 2023 - Technologies of Decision Support and Proportionali.pdf\n",
      "files/1488/Blauth et al. - 2022 - Artificial Intelligence Crime An Overview of Mali.pdf\n",
      "files/1481/Al-Hussaini et al. - 2022 - Seeking Human Help to Manage Plan Failure Risks in.pdf\n",
      "files/1475/ŞİMŞEK_KİRİSCİ_2023_A NEW RISK ASSESSMENT METHOD FOR AUTONOMOUS VEHICLE DRIVING SYSTEMS.pdf\n",
      "files/1472/Sebastian_2023_Do ChatGPT and Other AI Chatbots Pose a Cybersecurity Risk.pdf\n",
      "files/1486/Bächle and Bareis - 2022 - “Autonomous weapons” as a geopolitical signifier i.pdf\n",
      "files/1454/Johnson_2022_Delegating strategic decision-making to machines.pdf\n",
      "files/1453/Sun et al_2023_CANARY.pdf\n",
      "files/1465/Lekea et al_2023_Exploring Enhanced Military Ethics and Legal Compliance through Automated.pdf\n",
      "files/1496/Veluwenkamp - 2022 - Reasons for Meaningful Human Control.pdf\n",
      "files/1462/BOTEZATU_2023_AI-Centric secure outer space operations.pdf\n",
      "files/1463/Harshith et al_2023_Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks.pdf\n",
      "files/1464/Kereopa-Yorke_2023_ClausewitzGPT_Framework.pdf\n",
      "files/1452/Oimann_2023_The Responsibility Gap and LAWS.pdf\n",
      "files/1455/Jorge Peña Queralta et al_2022_Secure Encoded Instruction Graphs for End-to-End Data Validation in Autonomous.pdf\n",
      "files/1439/Meerveld et al. - 2023 - The irresponsibility of not using AI in the milita.pdf\n",
      "files/1527/Roden-Bow - 2023 - Killer Robots and Inauthenticity A Heideggerian R.pdf\n",
      "files/1516/Syse and Cook - 2023 - Robotic Virtue, Military Ethics Education, and the.pdf\n",
      "files/1511/Umbrello - 2022 - Editorial for the Special Issue on Meaningful Huma.pdf\n",
      "files/1510/Hoffmann and Frase - 2023 - Adding Structure to AI Harm.pdf\n",
      "files/1528/Blanchard and Taddeo - 2022 - Jus in bello Necessity, The Requirement of .pdf\n",
      "files/1738/Pan and Mishra - 2023 - AI Trojan Attack for Evading Machine Learning-base.pdf\n",
      "files/1737/Dorton and Harper - 2022 - A Naturalistic Investigation of Trust, AI, and Int.pdf\n",
      "files/1448/Tóth et al_2022_The Dawn of the AI Robots.pdf\n",
      "files/1477/Hunter and Bowen - 2023 - We’ll never have a model of an AI major-general A.pdf\n",
      "files/1483/Goldfarb and Lindsay - 2022 - Prediction and Judgment Why Artificial Intelligen.pdf\n",
      "files/1484/Maathuis - 2022 - On Explainable AI Solutions for Targeting in Cyber.pdf\n",
      "files/1470/Sangwan et al_2023_Cybersecurity for AI Systems.pdf\n",
      "files/1479/Luo - 2022 - Addressing military AI risks in U.S.–China crisis .pdf\n",
      "files/1446/Hagos_Rawat_2022_Recent Advances in Artificial Intelligence and Tactical Autonomy.pdf\n",
      "files/1478/Bode et al. - 2023 - Prospects for the global governance of autonomous .pdf\n",
      "files/1447/Johnson_2022_The AI Commander Problem.pdf\n",
      "files/1471/Santoso_Finn_2023_An In-Depth Examination of Artificial Intelligence-Enhanced Cybersecurity in.pdf\n",
      "files/1485/Devitt - 2023 - Bad, mad, and cooked Moral responsibility for civ.pdf\n",
      "files/1449/Vyhmeister et al_2022_A responsible AI framework.pdf\n",
      "files/1482/Hamad and Steinhorst - 2023 - Security Challenges in Autonomous Systems Design.pdf\n",
      "files/1476/2023_Autonomous Weapon Systems.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the PDF dataset\n",
    "pdf_data = {}\n",
    "for pdf_path, label in load_dataset():\n",
    "    text, word_counts, phrase_counts = process_pdf(pdf_path)\n",
    "    pdf_data[pdf_path] = (text, word_counts, phrase_counts, label)\n",
    "\n",
    "    # Generate and display word cloud\n",
    "    #generate_wordcloud(text, label)  # Use `label` for filename reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fcff600-ad35-4294-9bc6-2b008c93a2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter key phrases (comma-separated) to search for:  model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'PyPDF2.errors' has no attribute 'PdfReaderError'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36mprocess_pdf\u001b[0;34m(pdf_path, password)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Extract text from PDF\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m     reader \u001b[38;5;241m=\u001b[39m PdfReader(f, password\u001b[38;5;241m=\u001b[39mpassword)  \u001b[38;5;66;03m# Provide password if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/2024env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m user_phrases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter key phrases (comma-separated) to search for: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m user_phrases \u001b[38;5;241m=\u001b[39m user_phrases\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Split the input string into a list of phrases\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m relevant_pdfs \u001b[38;5;241m=\u001b[39m \u001b[43mfind_relevant_pdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_phrases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relevant_pdfs:\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPDFs most relevant to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_phrases\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 155\u001b[0m, in \u001b[0;36mfind_relevant_pdfs\u001b[0;34m(pdf_paths, search_terms)\u001b[0m\n\u001b[1;32m    153\u001b[0m relevant_pdfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf_path \u001b[38;5;129;01min\u001b[39;00m pdf_paths:\n\u001b[0;32m--> 155\u001b[0m     text, word_counts, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Phrase counts not used here, optional for future use\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[1;32m    157\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 42\u001b[0m, in \u001b[0;36mprocess_pdf\u001b[0;34m(pdf_path, password)\u001b[0m\n\u001b[1;32m     39\u001b[0m     phrase_counts \u001b[38;5;241m=\u001b[39m Counter(ngrams(words, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# Count bigrams (2-word phrases)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text, word_counts, phrase_counts\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIOError\u001b[39;00m, \u001b[43mPyPDF2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPdfReaderError\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing PDF \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'PyPDF2.errors' has no attribute 'PdfReaderError'"
     ]
    }
   ],
   "source": [
    "user_phrases = input(\"Enter key phrases (comma-separated) to search for: \")\n",
    "user_phrases = user_phrases.split(\",\")  # Split the input string into a list of phrases\n",
    "\n",
    "relevant_pdfs = find_relevant_pdfs(user_phrases, pdf_data)\n",
    "\n",
    "if relevant_pdfs:\n",
    "  print(f\"\\nPDFs most relevant to '{user_phrases}':\")\n",
    "  sorted_results = sorted(relevant_pdfs.items(), key=lambda x: x[1], reverse=True)\n",
    "  for pdf_path, score in sorted_results:\n",
    "    print(f\"- {pdf_path} (Score: {score})\")\n",
    "else:\n",
    "  print(f\"No PDFs found containing any of the phrases: {user_phrases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1f0c9-899a-4ce4-a116-3d3a38174ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
